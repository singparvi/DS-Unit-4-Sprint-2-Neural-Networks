{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"InClass_LS_DS_421_Architect_Lecture.ipynb","provenance":[],"collapsed_sections":["MDveyRYCOQTa","GfZ-DkE2OQTa","rEThlcbkEHHT","doDjyANkEmcG","Q2d6IwFNFEnx","Kc9Z66vHOQTb","oYZ3c3nmFdw6","VEMjxPiGDWkx","bMvL4D3mDWk5","L2EW1FGMDWlE","vOavpBcBDWlL","ziRD3mfDDWlO","Elv2gJDpDWlQ","5p8zODXpDWlS","4HXj5hgrRLVu","jcFtoqlDRLV_","E5-wkFBWRLWK","RtXWHxjWRLWV"]},"kernelspec":{"display_name":"py37  (Python3)","language":"python","name":"py37"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"toc-autonumbering":false,"toc-showcode":false,"toc-showmarkdowntxt":false},"cells":[{"cell_type":"markdown","metadata":{"id":"iAJaRSseDCrU"},"source":["Lambda School Data Science\n","\n","*Unit 4, Sprint 2, Module 1*\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"G3nUN1Sb0ueP","toc-hr-collapsed":true},"source":["# Architect (Prepare)\n","__*Neural Network Foundations*__"]},{"cell_type":"markdown","metadata":{"id":"esemlZKH0ueQ"},"source":["## Learning Objectives\n","* <a href=\"#p1\">Part 1</a>: Student should be able to describe the foundational components of a neural network\n","* <a href=\"#p2\">Part 2</a>: Student should be able to introduce the Keras Sequential Model API\n","* <a href=\"#p3\">Part 3</a>: Student should be able to learn how to select a model architecture\n","\n","Neural Networks are a whole new area of study and application that can be intimidating, but which represents some of the most powerful tools and techniques that we possess in machine learning today. In spite of the hype surrounding these topics I hope that you will come to see them as just another tool in your tool bag with their own strengths and weaknesses. They are useful, but they are not a silver bullet, and they are not always preferable to other -perhaps more simple- machine learning methods. \n","\n","The goal of this week is to familiarize you with the fundamental theory, terminology and libraries that will enable you to approach different neural network architectures. This week will not be a run-through of the history of Neural Networks and each of the individual advancements leading up to current technologies -we don't have time for that. We will spend some time on some older methods, but only to the degree that they will help introduce us to relevant terminology and understand more complex versions of these technologies."]},{"cell_type":"markdown","metadata":{"id":"dGb0yyBtBCBD","toc-hr-collapsed":false},"source":["# Foundational Neural Network Components (Learn)\n","<a id=\"p1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"4bG2tO3M0uen"},"source":["## Overview\n","\n","### Major Components\n","- Weights & Bias\n","- Neurons\n","- Activation Functions\n","- Layers\n","- Loss Function\n","\n","Let's zoom in on one of those components, arguably the most important component of all - the neuron."]},{"cell_type":"markdown","metadata":{"id":"PVt3Oz4gOQTX"},"source":["----\n","\n","### Dissecting the Artificial Neuron (a.k.a Perceptron)\n","\n","Every branch of science has a fundamental unit, a baseline model of a physical system, that is used as the starting point (the first priciple) of that science. Every idea, experiment result, and hypothesis in a branch of science rest upon the building block of that science; unless you're doing purely theoretical work that is explicitly looking to introduce a new builing block, challenge the first priciples. So it's important that we understand the builidng block of any science that we wish to study. \n","\n","In **Physics**, the fundamental building block is the **particle**.\n","\n","In **Chemistry**, the fundamental building block is the **chemical element**. \n","\n","In **Biology**, the fundamental building block is the **cell**.\n","\n","In **Neuroscience**, the fundamental building block is the **Neuron**.\n","\n","**You are about to learn computational Neuronscience!** \n","\n","This image has a side-by-side comparison of a biological model of a neuron and a computational model of a neuron.\n","![](https://miro.medium.com/max/610/1*SJPacPhP4KDEB1AdhOFy_Q.png)\n","\n","\n","Let's dive deeper into the computationl model of the neuron. \n","![](https://images.squarespace-cdn.com/content/v1/59d9b2749f8dce3ebe4e676d/1547561883197-ZO8CJILFNGZMORIJZOJ1/ke17ZwdGBToddI8pDm48kAuxETKhxDsgKuKi-UGpnEIUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcofcaXKJYLRX9EzsQWIgTsTayKHY9LJ3-BRv5jWxoI5y-JkyZtODn1AMLPOg8sn20/Artificial-Neuron.png )\n","\n","**Note:** The $\\mathbf{T}$ superscript on the $\\mathbf{w}$ weight vector stands for a vector/matrix transpose. Sometimes these are necessary in order to get the dimensions of a vector or matrix product to align so that a valid product can take place. \n","\n","**This is a Very Important Equation!**\n","\n","This is the neuron in a single equation, all the relevant terms are preseant. This equation will continue to reappear as we continue our study of various neural network architectures, gradient descent, and back-propagation. \n","\n","\n","$$\\mathbf{y = f(w^T x + b )}$$"]},{"cell_type":"markdown","metadata":{"id":"t4YK5EsTOQTX"},"source":["----\n","### Build a perceptron from scratch!\n","\n","First let's define some terms!\n","\n","Let $\\mathbf {X}$ be defined as the set of all vectors in an $\\mathbf{N}$-dimensional vector space denoted as $\\mathbb{R}^{N}$ \n","\n","Let $\\mathbf {x}$ be a vector from $\\mathbf {X}$ such that $\\mathbf {x} \\in \\mathbf {X}$\n","\n","Let $\\mathbf {W}$ be defined as the set of all weight vectors in $\\mathbb{R}^{N}$  \n","\n","Let $\\mathbf {w}$ be a vector from $\\mathbf {W}$ such that $\\mathbf {w} \\in \\mathbf {W}$\n","\n","Let $\\mathbf {b}$ be a scaler from $\\mathbb{R}$ "]},{"cell_type":"code","metadata":{"id":"DJJofc4DOQTX","executionInfo":{"status":"ok","timestamp":1621387551851,"user_tz":240,"elapsed":739,"user":{"displayName":"Parvi Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicHh9ECKPiyR68S3IUC3uvEPxQPh88MULwl8RHhQ=s64","userId":"15870809085498692847"}}},"source":["import numpy as np\n","\n","# Although the math holds for any N-dim vector, let's keep things simple\n","\n","# define our 2-dim input vectors and input matrix \n","x1 = np.array([10, 20])\n","x2 = np.array([-10, -20])\n","x3 = np.array([100, 111])\n","\n","X = np.array([x1, \n","              x2, \n","              x3])\n","\n","# define our 2-dim weight vector\n","w = np.array([.2, .4]) \n","\n","# define out bias term \n","b = 1"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFKjgxW9OQTY"},"source":["In order to calculate a **weighted sum**, we can calculate a dot product between each pair of input and weight vectors \n","\n","$${\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} }~~=~~{\\displaystyle \\sum _{i=1}^{m}w_{i}x_{i}}$$"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3156efcfac56c547888ab75f5f05ad3b","grade":false,"grade_id":"cell-ae686cf824490f4f","locked":false,"schema_version":3,"solution":true,"task":false},"id":"OW8XMf2rOQTY"},"source":["def calc_weighted_sum(x, w):\n","    \"\"\"Calculates a matrix and vector product\"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhsoUO5HOQTY"},"source":["Next, we declare that there exist a function such that its output ranges from [0, 1] and that \n","\n","$${\\displaystyle f(\\mathbf {x} )={\\begin{cases}1&{\\text{if }}\\ \\mathbf {w} \\cdot \\mathbf {x} +b>0,\\\\0&{\\text{otherwise}}\\end{cases}}}$$\n","\n","\n","\n","Fortunately, there is a function that we can use for this and it's called the [**Sigmoid**](https://en.wikipedia.org/wiki/Sigmoid_function)"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"375bfc9e6bea83c59ac9de18a0380866","grade":false,"grade_id":"cell-eff2dc3416d41e68","locked":false,"schema_version":3,"solution":true,"task":false},"id":"tj7v9CmBOQTZ"},"source":["def sigmoid(w_sum, b):\n","    \"\"\"Calclate the output of a sigmoid \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJxBYWnsOQTZ"},"source":["$$\\textbf{Sigmoid in its algebraic expression}$$\n","$${\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}}$$\n","\n","\n","$$\\textbf{Sigmoid in its geometric expression}$$\n","\n","![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)\n","\n","By setting a threshold value of 0.5, we can ensure that our function returns a 1 for positive input values and a 0 for negative input values. "]},{"cell_type":"code","metadata":{"id":"dk3WPscmOQTZ"},"source":["# calculate weighted sum value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7250IJP-OQTZ"},"source":["# calculate output of activation function"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"f46f3fe25e6e9326649ac138dc6025c4","grade":false,"grade_id":"cell-26e6dfe6d2fcdd5e","locked":false,"schema_version":3,"solution":true,"task":false},"id":"i_1CsfBCOQTZ"},"source":["# put it all together \n","def perceptron(w, X, b):\n","    \"\"\"\n","    Calculates the sigmoid of a weighted sum plus a bias term f( w * x + b)\n","    and returns a classification for the input data (i.e. a prediction)\n","    \n","    Returns a 1 if sigmoid output is greated than the threshold\n","    Returns a 0 if sigmoid output is less than the threshold\n","    \n","    Parameters\n","    ----------\n","    w: numpy array \n","        weight vector\n","        \n","    X: numpy 2D array\n","        Input data \n","        \n","    b: scalar (i.e. constant)\n","        Bias term \n","        \n","    Returns \n","    -------\n","    boolean vlaue \n","    \"\"\"\n","    \n","    # YOUR CODE HERE\n","    raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4liJSq_OQTa"},"source":["b = 1 # this is the original value of our bias \n","perceptron(w, X, b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItlXMZHbOQTa"},"source":["b = -70 # what happens when we change the bias to a large negative number ?\n","perceptron(w, X, b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDveyRYCOQTa"},"source":["-----\n","\n","### Question: What kind of predictive model is the perceptron?\n","\n","The perceptron is a **linear binary classifier** - just like Logistic Regression \n","\n","**Linear** in the sense that it can only create linear decison bounaries between data points. \n","\n","**Binary** in the sense that it can only distinguish between two classes in a classification task. \n","\n","By combinding multiple neurons into a neural network we can overcome both of these limitations. More on this in a bit. For now, let's zoom back out and look at the big picture again. \n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"-9x3x5XgtD3i","toc-hr-collapsed":true},"source":["## Back to the big picture\n","\n","<img src=\"IMG_0167.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"GfZ-DkE2OQTa"},"source":["### Layers\n","A single *dense* layer is a collection of neurons. There are three common types of layers:\n","- Input\n","- Hidden\n","- Output"]},{"cell_type":"markdown","metadata":{"id":"rEThlcbkEHHT"},"source":["### Weights\n","Considered a parameter within our neural network. In the diagram above, you can consider a weight an arrow. During the training process, the weights are adjusted to minimize loss."]},{"cell_type":"markdown","metadata":{"id":"doDjyANkEmcG"},"source":["### Bias\n","Considered a parameter within our neural network. The bias term is a constant allowing greater flexibility in the output of a neuron."]},{"cell_type":"markdown","metadata":{"id":"Q2d6IwFNFEnx"},"source":["### Neurons\n","Often called a \"unit\" which is shorthand for \"activation unit\". Neurons caculate the weighted sum of the inputs plus the bias term and pass the values thru an activation function. "]},{"cell_type":"markdown","metadata":{"id":"Kc9Z66vHOQTb"},"source":["### Loss Functions\n","The function that informs the updating of weights via gradient descent. You want to minimize this function during training.  "]},{"cell_type":"markdown","metadata":{"id":"oYZ3c3nmFdw6"},"source":["### Activation Functions\n","Controls the output of any given neuron. Activation functions most important feature is its derivative or slope. The derivative is the portion that affects updating the weights during model training, **i.e. Gradient Descent**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f8QnjfTHCCwr"},"source":["\n","**Sigmoid Curve and its Derivative**\n","\n","![](https://i.stack.imgur.com/inMoa.png)"]},{"cell_type":"markdown","metadata":{"id":"AW5rujQ4DWkv"},"source":["# Keras Sequential API (Learn)"]},{"cell_type":"markdown","metadata":{"id":"FdqgNz_ADWkw","toc-hr-collapsed":true},"source":["## Overview\n","\n","> \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:\n","\n","> Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n","Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n","Runs seamlessly on CPU and GPU.\" "]},{"cell_type":"markdown","metadata":{"id":"VEMjxPiGDWkx"},"source":["### Keras Perceptron Sample"]},{"cell_type":"code","metadata":{"id":"hm7YHCA3DWkx"},"source":["import pandas as pd\n","\n","data = { 'x1': [0,1,0,1],\n","         'x2': [0,0,1,1],\n","         'y':  [0,1,1,0]\n","       }\n","\n","df = pd.DataFrame.from_dict(data).astype('int')\n","X = df[['x1', 'x2']].values\n","y = df['y'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cpHYbznNahw-"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zW_GYH3DWk0"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"X7OsYwmPCCwr","nbgrader":{"cell_type":"code","checksum":"94dc769fcb59dcac32fcf0b30fa0a8d7","grade":false,"grade_id":"cell-61f9da5f7fa91510","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["## This is our perceptron \n","\n","# instantiate a sequential model\n","\n","# add a dense layer \n","\n","# complie the model \n","\n","# fit the model \n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQ-KdVJ5DWk2"},"source":["# evaluate the model\n","scores = model.evaluate(X, y)\n","print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIRJQfX2YQMp"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPRBBrjYOQTc"},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# notice that we can't just draw a line to seperate the two classes\n","# that's because perceptrons are essentially logistic regression when we choose the sigmoid as our activation function\n","# logisitic regression are linear functions that assume that data is linearly distributed \n","# our data is not linearly distributed \n","sns.relplot(x=\"x1\", y=\"x2\", hue=\"y\",\n","            sizes=(40, 400), alpha=1, palette=\"muted\",\n","            height=6, data=df);\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U0e0tDhHOQTc"},"source":["The perceptron is limited to linearly separable data sets. But if we combined them in a multi-perception model (i.e. a neural network) then they can handle non-linear data!\n","\n","![](https://www.edureka.co/blog/wp-content/uploads/2017/07/Linear-528x264.jpg)"]},{"cell_type":"code","metadata":{"id":"iVMEuGzCOQTc"},"source":["# this video shows the Kernal Trick for SVM\n","# but neural networks (i.e. multi-perceptrons) apply the exact same concept \n","from IPython.display import YouTubeVideo\n","YouTubeVideo('ndNE8he7Nnk')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4zli_eR3OQTd"},"source":["**Through the mathematics of Linear Algebra, we can understand what is happending.**\n","\n","We have a vector from a lower dimensional space that is operated on by a function that transforms it into a vector in a higher dimensional space. The geometry of those data points changes and when the right transformation is applyed, the data becomes seperable by a linear boundery. \n","\n","In neural netowrks, when data gets pass from one layer of neurons to another layer of neurons, the exact same type of transformation is happening - data is getted tossed around between vector spaces of various dimensions. \n","\n","Each hidden layer in the NN represents a vector space of some dimensionality. \n","\n","[**Check out this interactive tool to make this idea a bit more concrete**](http://alexlenail.me/NN-SVG/index.html)"]},{"cell_type":"markdown","metadata":{"id":"0TfFvipdDWk4","toc-hr-collapsed":true},"source":["## Follow Along\n","\n","In the `Sequential` api model, you specify a model architecture by 'sequentially specifying layers. This type of specification works well for feed forward neural networks in which the data flows in one direction (forward propagation) and the error flows in the opposite direction (backwards propagation). The Keras `Sequential` API follows a standardarized worklow to estimate a 'net: \n","\n","1. Load Data\n","2. Define Model\n","3. Compile Model\n","4. Fit Model\n","5. Evaluate Model\n","\n","You saw these steps in our Keras Perceptron Sample, but let's walk thru each step in detail."]},{"cell_type":"markdown","metadata":{"id":"bMvL4D3mDWk5","toc-hr-collapsed":false},"source":["### Load Data"]},{"cell_type":"code","metadata":{"id":"pnk2YNVODWk5"},"source":["from tensorflow import keras\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D27iFEuRDWk7"},"source":["# Load the Data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbZ3MTDCb3mW"},"source":["import matplotlib.pyplot as plt\n","\n","plt.imshow(X_train[10]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"gpIYmn6MDWlC","nbgrader":{"cell_type":"code","checksum":"f4ec7033e74f9668ebf3e31a97fbaf1b","grade":false,"grade_id":"cell-1bd3c5df73f77628","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# normalize the pixel values \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"XJGguQn3dAKj","nbgrader":{"cell_type":"code","checksum":"381cfcec4b6145530df8aa43fa48ad2d","grade":false,"grade_id":"cell-54c049d576c55a6f","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# flatten image \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2EW1FGMDWlE"},"source":["### Define Model"]},{"cell_type":"code","metadata":{"id":"m5IHQYTbDWlE"},"source":["from tensorflow.keras import Sequential"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CEXDUmUfDWlG"},"source":["I'll instantiate my model as a \"sequential\" model. This just means that I'm going to tell Keras what my model's architecture should be one layer at a time."]},{"cell_type":"code","metadata":{"id":"hnGHcljbDWlH"},"source":["# https://keras.io/getting-started/sequential-model-guide/\n","model = Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dDaQJ3wZDWlJ"},"source":["Adding a \"Dense\" layer to our model is how we add \"vanilla\" perceptron-based layers to our neural network. These are also called \"fully-connected\" or \"densely-connected\" layers. They're used as a layer type in lots of other Neural Net Architectures but they're not referred to as perceptrons or multi-layer perceptrons very often in those situations even though that's what they are.\n","\n"," > [\"Just your regular densely-connected NN layer.\"](https://keras.io/layers/core/)\n"," \n"," The first argument is how many neurons we want to have in that layer. To create a perceptron-esque model we will just set it to 10. Our architecture is just an input and output layer. We will tell it that there will be 784 inputs coming into this layer from our dataset and set it to use the sigmoid activation function."]},{"cell_type":"code","metadata":{"deletable":false,"id":"e7K6IjqZDWlJ","nbgrader":{"cell_type":"code","checksum":"744b7928d170a590f22d8058698075d3","grade":false,"grade_id":"cell-fa08b4e31e0b7849","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# add a hidden layer \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"CtDiAZ2DeYch","nbgrader":{"cell_type":"code","checksum":"69f2bab23ffbe7d18434384195cc4394","grade":false,"grade_id":"cell-4ee33acb86df23bc","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# add an output layer with softmax\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDWlyt2aexJj"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOavpBcBDWlL","toc-hr-collapsed":false},"source":["### Compile Model\n","Using binary_crossentropy as the loss function here is just telling keras that I'm doing binary classification so that it can use the appropriate loss function accordingly. If we were predicting non-binary categories we might assign something like `categorical_crossentropy`. We're also telling keras that we want it to report model accuracy as our main error metric for each epoch. We will also be able to see the overall accuracy once the model has finished training.\n","\n","#### Adam Optimizer\n","Check out this links for more background on the Adam optimizer and Stohastic Gradient Descent\n","* [Adam Optimization Algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n","* [Adam Optimizer - original paper](https://arxiv.org/abs/1412.6980)"]},{"cell_type":"code","metadata":{"id":"ECMdYu63DWlM"},"source":["model.compile(optimizer='adam', \n","              loss='sparse_categorical_crossentropy', \n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziRD3mfDDWlO","toc-hr-collapsed":false},"source":["### Fit Model\n","\n","Lets train it up! `model.fit()` has a `batch_size` parameter that we can use if we want to do mini-batch epochs, but since this tabular dataset is pretty small we're just going to delete that parameter. Keras' default `batch_size` is `None` so omiting it will tell Keras to do batch epochs."]},{"cell_type":"code","metadata":{"id":"ZIOYRNyyDWlO"},"source":["results = model.fit(X_train, \n","                    y_train, \n","                    epochs=5, \n","                    validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YzUK4Mfh4An"},"source":["import seaborn as sns\n","\n","epochs = [i for i in range(len(results.history['loss']))]\n","sns.lineplot(epochs, results.history['loss'], label=\"train\")\n","sns.lineplot(epochs, results.history['val_loss'], label=\"test\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Elv2gJDpDWlQ"},"source":["### Evaluate Model"]},{"cell_type":"code","metadata":{"id":"vmF6OL6UDWlR"},"source":["model.evaluate(X_test,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5p8zODXpDWlS"},"source":["### Unstable Results\n","\n","You'll notice that if we rerun the results might differ from the origin run. This can be explain by a bunch of factors. Check out some of them in this article: \n","\n","<https://machinelearningmastery.com/randomness-in-machine-learning/>"]},{"cell_type":"markdown","metadata":{"id":"EULJBiUZDWlT"},"source":["## Challenge\n","\n","You will be expected to leverage the Keras `Sequential` api to estimate a feed forward neural networks on a dataset.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ueDVpctAzvy8"},"source":["# Choosing Architecture (Learn)"]},{"cell_type":"markdown","metadata":{"id":"KDFKiEkkRLU1"},"source":["## Overview\n","\n","Choosing an architecture for a neural network is almost more an art than a science. The best way to choose an architecture is through research and experimentation. \n","\n","Let's do a few experiments, and track our results using a tool called TensorBoard which is a way to interactively visualize the results of our various experiences. Here is our previous model with TensorBoard incorporated: "]},{"cell_type":"code","metadata":{"id":"GbEPV2H0RLVJ"},"source":["%load_ext tensorboard\n","\n","import os\n","import datetime\n","import tensorflow as tf\n","\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"DaHvGD7dRLVf","nbgrader":{"cell_type":"code","checksum":"1ca239a26f1f1f368074e6fc8d6b3ea5","grade":false,"grade_id":"cell-676b61a3530bd003","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["## build a 1 layer neural network \n","\n","# instantiate a sequential model\n","\n","# add a dense layer\n","\n","# add an output layer\n","\n","# complie the model \n","\n","# fit the model \n","\n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BC3ZPfRRLVn"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Sb7O8sVRLVt"},"source":["## Follow Along\n","\n","Let's run a couple of experiments in groups based on your birthday: \n","1. Jan - March:  Try adding an additional layer to the model\n","2. April - June: Add 2 hidden layers with identical number of neurons\n","3. July - Sept: Change the activation functions in the hidden layers (used as many layers as you want)\n","4. Oct - December: Try changing the optimization function and use any architecture that you want. "]},{"cell_type":"markdown","metadata":{"id":"4HXj5hgrRLVu"},"source":["### Additional Hidden Layer"]},{"cell_type":"code","metadata":{"deletable":false,"id":"TpotMnV5RLVw","nbgrader":{"cell_type":"code","checksum":"1a2b8e229e4214a308a86c78db60d664","grade":false,"grade_id":"cell-0697978adf354af0","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["## build a single hidden layer neural network \n","\n","# instantiate a sequential model\n","\n","# add 1st dense layer\n","\n","# add 2nd dense layer\n","\n","# add an output layer\n","\n","# complie the model \n","\n","# fit the model \n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmWpIWqlRLV6"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcFtoqlDRLV_"},"source":["### 2 Additional Hidden Layers"]},{"cell_type":"code","metadata":{"deletable":false,"id":"FHXfIJwKZBkE","nbgrader":{"cell_type":"code","checksum":"e8e6208c1761646cbdf6b29e5c583319","grade":false,"grade_id":"cell-9dff4977bb8f9c10","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["## build a 3 hidden layer neural network \n","\n","# instantiate a sequential model\n","\n","# add 1st dense layer\n","\n","# add 2nd dense layer\n","\n","# add 3rd dense layer\n","\n","# add an output layer\n","\n","# complie the model \n","\n","# fit the model \n","\n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQrKE7KCRLWE"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E5-wkFBWRLWK"},"source":["### Different Activation Functions"]},{"cell_type":"code","metadata":{"deletable":false,"id":"80KQ9C5XRLWL","nbgrader":{"cell_type":"code","checksum":"8d6bc32a916aedf8add24ad2af90911f","grade":false,"grade_id":"cell-75ff0e7f5620ada6","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["## build a 3 hidden layer neural network with different activation functions\n","\n","# instantiate a sequential model\n","\n","# add 1st dense layer\n","\n","# add 2nd dense layer\n","\n","# add 3rd dense layer\n","\n","# add an output layer\n","\n","# complie the model \n","\n","# fit the model \n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wd2wnkyjRLWP"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RtXWHxjWRLWV"},"source":["### Different Optimization Functions"]},{"cell_type":"code","metadata":{"deletable":false,"id":"6nnMIhSyZIDP","nbgrader":{"cell_type":"code","checksum":"706f7679f7d118f179a91d0942482f7c","grade":false,"grade_id":"cell-4fb011c210ce491a","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["## build a 3 hidden layer neural network with a different optimizer\n","\n","# instantiate a sequential model\n","\n","# add 1st dense layer\n","\n","# add 2nd dense layer\n","\n","# add 3rd dense layer\n","\n","# add an output layer\n","\n","# complie the model \n","\n","# fit the model \n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k32YJ0G4RLWW"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WaMLtERVRLWb"},"source":["## Challenge\n","\n","You will have to choose your own architectures in today's module project. In the next module, we will discuss hyperparameter optimization which can help you handle these numerous choices. \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ucUP72uiyQ_0","toc-hr-collapsed":false},"source":["# Sources\n","Knowledge doesn't come out of a vacuum. Neither does our code. We build off the work of other incredibly intelligent and harding work people. The academic and impementation sections are our way of saying **Thank You** to them. The external review material is stuff we've watched or read in the past we think could also help you. \n","\n","## Academic References\n","(i.e. Theory and research we referenced in preparing this content)\n","- McCulloch, W.S. & Pitts, W. Bulletin of Mathematical Biophysics (1943) 5: 115. https://doi.org/10.1007/BF02478259\n","- Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386â€“408. https://doi.org/10.1037/h0042519\n","- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning.\n","\n","## Implementation References\n","(i.e. Stuff we used / referenced to make the code in this notebook)\n","- [NN-SVG](http://alexlenail.me/NN-SVG/index.html) by Alex Lenail. Used to generate diagrams for this notebook. \n","- Alammar, Jay (2016). The Illustrated Transformer [A Visual and Interactive Guide to the Basics of Neural Networks](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/).\n","- [SINGLE LAYER NEURAL NETWORK - PERCEPTRON MODEL ON THE IRIS DATASET USING HEAVISIDE STEP ACTIVATION FUNCTION](https://www.bogotobogo.com/python/scikit-learn/Perceptron_Model_with_Iris_DataSet.php) by K Hong. For Perceptron Demo.\n","\n","## External Review Material\n","(i.e. Stuff we recommend watching to go to the next level of understanding)\n","\n","- [3 Blue 1 Brown Neural Network Videos](https://youtu.be/aircAruvnKk)\n","- [Andrew Ng Neural Network Introduction Videos](https://www.youtube.com/watch?v=1ZhtwInuOD0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=43)"]},{"cell_type":"code","metadata":{"id":"XiCNArbjCMvq"},"source":[""],"execution_count":null,"outputs":[]}]}