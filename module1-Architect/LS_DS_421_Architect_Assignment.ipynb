{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"LS_DS_421_Architect_Assignment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ObyHCH8HvHSf"},"source":["\n","# *Data Science Unit 4 Sprint 2 Assignment 1*\n","\n","Use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n","\n","### Objective\n","\n","- Build a baseline classification model then run a few experiments with different optimizers and learning rates. \n","- Don't forgot to [**switch to GPU on if you're running your notebook on Colab!**](https://colab.research.google.com/notebooks/gpu.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"s-Tc3ovEyQ9b"},"source":["## Load Your Data"]},{"cell_type":"code","metadata":{"id":"CkU0pAYCvU8o"},"source":["# imports in first cell \n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","\n","# use Sequential to build out your model\n","from tensorflow.keras.models import Sequential\n","\n","# Dense layer is used for Fully Connected Forward Feeding networks\n","from tensorflow.keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pk__jpisBzqS"},"source":["# load in data set\n","data = np.load('../quickdraw10.npz')\n","X = data['arr_0']\n","y = data['arr_1']\n","\n","\n","print(X.shape)\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8qsDqdqvHDd"},"source":["class_names = ['apple',\n","             'anvil',\n","             'airplane',\n","             'banana',\n","             'The Eiffel Tower',\n","             'The Mona Lisa',\n","             'The Great Wall of China',\n","             'alarm clock',\n","             'ant',\n","             'asparagus']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Owbm1EbxvA5A"},"source":["# these are your 10 unique images\n","plt.figure(figsize=(10,5))\n","start = 0\n","\n","for num, name in enumerate(class_names):\n","    plt.subplot(2,5, num+1)\n","    plt.xticks([])\n","  plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(X[start].reshape(28,28), cmap=plt.cm.binary)\n","    plt.xlabel(name)\n","    start += 10000\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIcaR9LqBzqT"},"source":["Say hello to the Quickdraw dataset. \n","\n","You'll be using this dataset a lot this week as an alternative to the mnist which we'll be using in the guided projects. The nice thing about this dataset is that it's simple, which allows us to focus on our model, it's various components, and gradually coming to a better understand of how to build neural networks without worrying about cleaning and preping our image data too much. "]},{"cell_type":"code","metadata":{"id":"c97_M1WNvTNY"},"source":["# always a good idea to shuffle your dataset \n","X, y = shuffle(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jb70CbLVyK65"},"source":["-----\n","\n","## Build Your Baseline Model\n","\n","Make sure that you\n","\n","- **Determine** the dimensionality of your input data by investigating **X**\n","- **Normalize** your input data to values between 0 and 1 \n","- **Determine** the number of neurons in your output layer by investigating **Y**\n","- **Select** `sparse_categorical_crossentropy` as your loss function.\n","- **Select** `sgd` as your optimizer.\n","- **Add** 3 hidden layers to your model with the following number of nodes\n","    - h1 has 500 nodes\n","    - h2 has 250 nodes\n","    - h3 has 100 nodes\n","    \n","- **Set** epochs to 20 \n","- **Use** the `validation_split` command to automatically create a training / validation dataset from within the model, so you don't have to do it yourself.\n","    -  Specify a percentage such as .2 in your fit statement.\n"," \n"," \n","Not sure what the various parameters are for or what what values to assign to them?\n","\n","- Reference the guided project notebook for Sprint 2 Module 1\n","- Reference the [**Keras documentation**](https://keras.io/api/)\n","- Google other examples\n","- Discuss your results with classmates "]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"ef20dd34df6998e0a50e394d59d58659","grade":false,"grade_id":"cell-907b9348d7a2ebb3","locked":false,"schema_version":3,"solution":true,"task":false},"id":"Eha2jTbgBzqU"},"source":["# get dim of image row vectors and save to imput_dim\n","\n","# get number of unique labels and save to n_output_nodels\n","\n","# normalize image data to values between 0 and 1 (by dividing by max pixel value)\n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ib2xDjnBzqU"},"source":["# a check on your data prep \n","assert  X_scaled.max(), \"Max pixel value should be 1.0, make sure you normalize your data\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"zHWblzsMyNkU","nbgrader":{"cell_type":"code","checksum":"f0ba174cb72f491f73c3aa7df8ae7ac4","grade":false,"grade_id":"cell-b7c96fc46d86725f","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# instantiate a sequential object and call it model, then add layers to your model\n","\n","# add a compile layer but don't fit your model yet \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVdLrQekBzqU"},"source":["# a check on your model architecture \n","n_layers = len(model.get_config()[\"layers\"])\n","assert n_layers == 5, \"You should have 5 layers: input, h1, h2, h3, and output\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skAOijIrBzqU"},"source":["# check out your model summary \n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-YnfLooBzqV"},"source":["# fit your model and save training resuts to history \n","history = model.fit(X_scaled, y, \n","                    epochs=epochs, \n","                    # test set will be generated within the model\n","                    validation_split=0.2\n","                   )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0QJURWh-9uv"},"source":["----\n","### Visualize the results\n","\n","- Move results in `history` in a dataframe \n","- Use [**Seaborn**](https://seaborn.pydata.org/generated/seaborn.lineplot.html) to create lineplots for both loss and accuracy by epoch. \n","- Analyze the results and write a couple of obsverations. \n","\n","At what point should we have stopped training the model and why? "]},{"cell_type":"code","metadata":{"deletable":false,"id":"ijAlzfYKAFaY","nbgrader":{"cell_type":"code","checksum":"6ef8cde40701c2ef57cf853b19455125","grade":false,"grade_id":"cell-16e647cfc3291a01","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-kFtwZSBzqV"},"source":["# a check on our model training\n","assert df.shape[0] == 20, \"df should have the training results from 20 epochs\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"424d241660318a72ecda935be10485d7","grade":false,"grade_id":"cell-96dba18873c4cffc","locked":false,"schema_version":3,"solution":true,"task":false},"id":"fBxBuRewBzqW"},"source":["# use the plotting method in your dataframe to plot the modeling results \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJPFyZJ_BzqW"},"source":["### Observations \n","\n","Based on the plot of the training and validation loss answer the following questions. "]},{"cell_type":"markdown","metadata":{"id":"vHc_2WYyBzqW"},"source":["**Was our model able to learn over the course of the 20 epochs? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"b7713eb32ad3a910a58dfdb9494a9db3","grade":true,"grade_id":"cell-820990a8232a858a","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"vm_I2wOFBzqW"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"givG2-tFBzqW"},"source":["**Is our model overfitting? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"1d30c64465cd35081d70578c20ecf96d","grade":true,"grade_id":"cell-33868e7ef6e401b5","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"gOw-vmVHBzqW"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"laWIFtbXBzqW"},"source":["**Could the model score benefit from additional epochs? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"0fc2208e2c78461578fc7a05224066b8","grade":true,"grade_id":"cell-5c883f0a3161e469","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"upMU5ERaBzqX"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"MAhBrcE4yOZe"},"source":["-----\n","## Change Optimizers\n","\n","Let's compare model performance between difference optimizers. \n","- Build a new model, identical as the last one but using `adam` for the optimizer. \n","- Visualize the training results just as we did for the last model. \n","- Save modeling results to adam_history so we don't erase the results from the previous model run"]},{"cell_type":"code","metadata":{"deletable":false,"id":"jIW_spOZ0cxy","nbgrader":{"cell_type":"code","checksum":"6d30d8c6f4521861a77c9ef04dbde904","grade":false,"grade_id":"cell-56663c8a5e75b71f","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# train same model as above but use sgd if you used adam previously ( or use adam if you used sgd previously)\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRBuNfVoBzqX"},"source":["# a check that we're using the correct optimizer in this model\n","opt_name = model.optimizer.get_config()[\"name\"]\n","assert opt_name == \"Adam\", \"you need to use adam for the optimizer in this model.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"1a4c01aa7dbcc832d75b05bffaeaacc8","grade":false,"grade_id":"cell-20118d1646215346","locked":false,"schema_version":3,"solution":true,"task":false},"id":"UYzPSqn0BzqX"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gFAM6cgBzqX"},"source":["### Observations \n","\n","This plot should look very different from the previous plot. Based on the plot of the training and validation loss answer the following questions. "]},{"cell_type":"markdown","metadata":{"id":"CIPA_H_IBzqX"},"source":["**Is our model overfitting? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"fa4141497af675d227aa510feb8db732","grade":true,"grade_id":"cell-7e524ccac970c33e","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"S-nN9DRzBzqX"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"V2qqFQFmBzqX"},"source":["**Could our model's performance improve by training on more than 20 epochs? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"ae03b8b897eb5d052590574ab3bba839","grade":true,"grade_id":"cell-456c40e885652955","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"QZ9bMCjRBzqY"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"3CwOfg2qBzqY"},"source":["**Assuming that you want to stick to this model architecture, what can you do to avoid overfitting?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"9f5a9393314231d3ce253363c1df1223","grade":true,"grade_id":"cell-74d7def19c66702b","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"jLjizT4MBzqY"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"nTQvReYLBzqY"},"source":["------\n","# Conclusion \n","\n","What you have observed by comparing the two models that are identitical, except for the optimizer, is that the choice of the optimizer can have a very big influence in the training outcome of ML models in general and in a neural networks in particuar. \n","\n","You might not actually know \n","\n","- why changing the optimizer from Stocastic Gradient Descent over to Adam made a difference.\n","- how Gradient Desecent works \n","- or that Adam is known as Adaptive Gradient Descent (i.e. is it a different version of ordinary Gradient Descent). \n","\n","But that's ok - tomorrow's lesson will be a deep dive into Gradient Descent. You'll learn the theory of Gradient Descent, we'll code up Gradient Descent from scratch, and we'll talk about how Adam is different from ordinary Gradient Descent. "]},{"cell_type":"markdown","metadata":{"id":"-cTVXjZVBzqY"},"source":["-----\n","# Preparation for Tomorrow \n","\n","In Preparation for tomorrow, you might need to watch a few videos. I say might because you may or may not already be comfortable with multi-variate calculus. \n","\n","### Theory of Calculus \n","The theory of Gradient Descent rest on [**the derivative from Calculus**](https://www.youtube.com/watch?v=WUvTyaaNkzM). If you've never taken a Calculus course before, or are a bit rusty, definitely watch this video in preparation for tomorrow's lesson. The take away here is to conceptually understand the derivative, you won't be asked to actually calculate any derivatives by hand. \n","\n","### The Gradient - a multi-dimensional derivative\n","Once you've watched the previous video, you are now set up to  understand how to conceptualize a derivative in an N-dimensional space (where N is any number ranging from 2, 3, 4, ..., all the way to a very large number of dimensions N ) [**This video visually explains the multi-dimensional derivative called the Gradient visually**](https://www.youtube.com/watch?v=GkB4vW16QHI) - and that's the take away. This point of this video is to help you understand the Gradient visually using 3D surfaces. You should also know that the Gradient is made of up partial derivatives, more on this tomorrow. \n","\n","### Contour Maps \n","It is very common to visual 3D surfaces as 2D contour maps - and we'll be making a lot of use of contour maps to understand Gradient Descent. Watch this video in order to [**understand the relationship between 3D surfaces and 2D contour maps**](https://www.youtube.com/watch?v=acdX4YamDtU)\n","\n","### The Calculus of Backpropagation \n","\n","Unlike standard Sklearn ML models, neural networks use not just Gradient Descent but also something called Backpropagatin in order to learn from the data. [**In order to understand how backpropagation works, you need to understand the Chain Rule in Calculus**](https://www.youtube.com/watch?v=acdX4YamDtU). The take away here is to understand how a partial derivative can be decomposed into a product of multiple derivatives. "]},{"cell_type":"markdown","metadata":{"id":"sKoqZO2hBzqY"},"source":["------\n","# Stretch Goals\n","\n","### This section is optional!\n","\n","Only after you've completed the above work and watched the recommended videos, are you then encouraged to experiment with building a few more models and analyze their results. \n","\n","Here are some suggestions to help you get started. \n","\n","- Train the same model using the Sigmoid activation fucntion and the Relu activation function and note the difference in their learning outcomes.  [**Keras docs on activation fucntions**](https://keras.io/api/layers/activations/). We'll cover the Relu activation function in Sprint 2 Module 3.\n","- Train the same model using Normalized and non-Normalized data and note the difference in their learning outcomes. \n","\n","\n","The objective here is to give you an oppertunity to get more practice with the Keras API (i.e. building models) and running a couple of experiments to help set you up for future lessons. In order words, we'll be discussing different activation functions and the affect of Normalized data on model training. "]},{"cell_type":"code","metadata":{"id":"KrcHqxFaBzqZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VT6yXlo5BzqZ"},"source":[""],"execution_count":null,"outputs":[]}]}